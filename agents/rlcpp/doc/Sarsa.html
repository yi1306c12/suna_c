<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>Sarsa</h2>
<a href="Q.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="ESarsa.html">Next</a>

<p>
Sarsa is similar to <a href="Q.html">Q-learning</a>, but uses the value of the actually performed action to determine its update, instead of the maximum available action. Its equation is:
</p>

<p>
<img src="../img/Sarsa.png" alt="Q_{t+1}(s_t,a_t) \overset{\alpha_t}{\longleftarrow} r_t + \gamma Q_t(s_{t+1},a_{t+1})" />
</p>

<h4>Neutral characteristics</h4>
<p>
<ul>
  <li>It is on-policy (this means its Q values approximate the value including the effects of exploration).</li>
  <li>It learns state-action values (Q values).</li>
</ul>
</p>

<h4>Advantages</h4>
<p>
<ul>
  <li>Tabular Sarsa can be shown to reach optimal solutions, when exploration decreases in a proper manner.</li>
  <li>Also for Sarsa, there has been much research and succesful applications with Q-learning.</li>
  <li>Has a more natural extension to eligibility traces than Q-learning.</li>
</ul>
</p>

<h4>Disadvantages</h4>
<p>
<ul>
  <li>Cannot handle continuous action spaces.</li>
  <li>Has higher variance in its updates than <a href="ESarsa.html">Expected-Sarsa</a>.</li>
</ul>
</p>

<a name="alg"></a>
<h4>Algorithm</h4>
<p>
  The Sarsa algorithm in schematic form:
</p>

<p>
  <img src="../img/Sarsaalg.png" alt="Sarsa algorithm" />
</p>

<p>
  Note that because knowledge of the next action is required, the algorithm is a little more complex than that of <a href="Q.html#alg">Q-learning</a> or <a href="ESarsa.html#alg">Expected-Sarsa</a>.
</p>

<p>
<b>Selected relevant publications:</b>
<ul>
  <li> G. Rummery  and  M. Niranjan (1994), On-line Q-learning using Connectionist systems, technical report no.166, University of Cambridge, Engineering Department.</li>
  <li> S. P. Singh, T. Jaakkola, M. L. Littman and C. Szepesvari (2000), "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms", Machine Learning, volume 38, number 3, pages 287-308, 2000.</li>
</ul>
</p>

<h3>Quick links:</h3>
<a href="Q.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="ESarsa.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
