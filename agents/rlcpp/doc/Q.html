<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>Q-learning</h2>
<a href="onlyQ.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="Sarsa.html">Next</a>

<p>
Q-learning is perhaps the most well known reinforcement learning algorithm. Its equation is:
</p>

<p>
<img src="../img/Q.png" alt="Q_{t+1}(s_t,a_t) \overset{\alpha_t}{\longleftarrow} r_t + \gamma \max_a Q_t(s_{t+1},a)" />
</p>

<h4>Neutral characteristics</h4>
<p>
<ul>
  <li>It is off-policy (this means that its Q values approximate the optimal Q values, regardless of exploration).</li>
  <li>It learns state-action values (Q values).</li>
</ul>
</p>

<h4>Advantages</h4>
<p>
<ul>
  <li>Tabular Q-learning can be shown to reach optimal solutions, even under continued exploration.</li>
  <li>Being the oldest of the considered algorithms, there has been much research and succesful applications with Q-learning.</li>
</ul>
</p>

<h4>Disadvantages</h4>
<p>
<ul>
  <li>Has been shown to sometimes diverge when function approximators are used.</li>
  <li>Cannot handle continuous action spaces.</li>
  <li>Has no natural extension to eligibility traces.</li>
</ul>
</p>

<a name="alg"></a>
<h4>Algorithm</h4>

<p>
  The Q-learning algorithm in schematic form:
</p>

<p>
  <img src="../img/Qalg.png" alt="Q-learning algorithm" />
</p>

<p>
  Compare with the similar, but different algorithms: <a href="QV.html#alg">QV-learning</a>, <a href="Sarsa.html#alg">Sarsa</a> and <a href="ESarsa.html#alg">Expected-Sarsa</a>.
</p>

<p>
<b>Selected relevant publications:</b>
<ul>
  <li> C. J. C. H. Watkins (1989), "Learning from Delayed Rewards".</li>
  <li> C. J. C. H. Watkins and P. Dayan (1992), "{Q}-Learning", Machine Learning Journal, volume 8, number 3/4, Special Issue on Reinforcement Learning, may 1992.</li>
</ul>
</p>

<h3>Quick links:</h3>
<a href="onlyQ.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="Sarsa.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
