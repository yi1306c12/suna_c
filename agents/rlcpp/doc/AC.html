<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>Actor-Critic Learning</h2>
<a href="QV.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="Acla.html">Next</a>

<p>
Actor-Critic learning as we define it here is an algorithm that uses state values to update state-dependent action values. Its equation for the action values is:
</p>

<p>
<img src="../img/AC1.png" alt="P_{t+1}(s_t,a_t) = P_t(s_t,a_t) + \alpha_t \Big( r_t + \gamma V_t(s_{t+1}) - V_t \Big)" />
</p>

<p>
We can also write this update in our normal notation that extends to function approximators, where we note that this update looks a little bit counter-intuitive:
</p>

<p>
<img src="../img/AC2.png" alt="P_{t+1}(s_t,a_t) \overset{\alpha_t}{\longleftarrow} r_t + \gamma V_t(s_{t+1}) - V_t(s_t) + P_t(s_t,a_t)" />
</p>

<h4>Neutral characteristics</h4>
<p>
<ul>
  <li>It is on-policy.</li>
  <li>Learns preference values that do not hold explicit information on the expected discounted rewards.</li>
</ul>
</p>

<h4>Advantages</h4>
<p>
<ul>
  <li>Using state values often speeds up learning.</li>
  <li>State values are easily extendable to eligibility traces.</li>
</ul>
</p>

<h4>Disadvantages</h4>
<p>
<ul>
  <li>Cannot handle continuous action spaces.</li>
</ul>
</p>

<a name="alg"></a>
<h4>Algorithm</h4>

<p>
  The Actor-Critic algorithm in schematic form:
</p>

<p>
  <img src="../img/ACalg.png" alt="Actor-Critic algorithm" />
</p>

<p>
<b>Selected relevant publications:</b>
<ul>
  <li> A.G. Barto, R.S. Sutton and C. Anderson (1983). Neuron-like adaptive elements that can solve difficult learning control problems, IEEE Transactions on Systems, Man, and Cybernetics, SMC-13: 834-846, 1983.</li>
  <li> R.S. Sutton and A.G. Barto (1998), Reinforcement Learning: An Introduction, MIT Press, 1998. </li>
</ul>
</p>

<h3>Quick links:</h3>
<a href="QV.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="Acla.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
