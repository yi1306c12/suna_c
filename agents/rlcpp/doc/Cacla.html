<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>Continuous Actor-Critic Learning Automaton (Cacla)</h2>
<a href="Acla.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- Next

<p>
Cacla can be viewed as the continuous version of the <a href="Acla.html">Acla</a> algorithm. The idea is that again state values are stored in a table or function approximation. However, instead of also storing state-action values, Cacla stores a single value for each state: an approximation for the optimal action. Of course, in problems with multi-dimensional actions, this can be a vector instead of a value.
</p>

<p>
The idea and implementation of Cacla is very simply. You get the output of your actor. Then, you explore around this value (for instance with gaussian exploration). Then, if the value of the state increases after performing the action, you update your actor towards this action:
</p>

<p>
  <img src="../img/V.png" alt="V_{t+1}(s_t)     \overset{\beta_t}{\longleftarrow}  r_t + \gamma V_t(s_{t+1})" />
</p>

<p>
  If
</p>

<p>
<img src="../img/Vinc.png" alt="V_{t+1}(s_t) > V_t(s_t)" />
</p>

<p>
  then
</p>

<p>
<img src="../img/Cacla.png" alt="A_{t+1}(s_t) \overset{\alpha_t}{\longleftarrow} a_t" />
</p>

<p>
If the state value decreases, the action was not such a good idea, and you do not update the actor.
</p>

<h4>Advantages</h4>
<p>
<ul>
  <li>Can handle both discrete and continuous action spaces.</li>
  <li>Does not require segmentation of state or action space.</li>
  <li>Fast and easy to implement.</li>
  <li>State values are easily extendable to eligibility traces.</li>
  <li>Has been shown to outperform several discrete algorithms (Q-learning, Sarsa, etc.) on problems with discrete action spaces</li>
  <li>Has been shown to outperform several continuous algorithms (Wire fitting, ADHDP) on problems with continuous action spaces.</li>
</ul>
</p>

<h4>Disadvantages</h4>
<p>
<ul>
  <li>May be less applicable to problems with a discrete, nominal action space.</li>
</ul>
</p>

<a name="alg"></a>
<h4>Algorithm</h4>

<p>
  The Cacla algorithm in schematic form:
</p>

<p>
  <img src="../img/Caclaalg.png" alt="Cacla algorithm" />
</p>

<p>
  Comparing the algorithm to <a href="Acla.html#alg">Acla</a> shows definite similarities. However, note that Cacla does not update when the state value decreases. This is different from Acla, but was shown to be a better choice for Cacla, since updating away from the action that was selected does not guarantee that you are updating towards an action that is better than the current output of the actor. For a more detailed discussion, see the second publication below. The second publication also shows variations of Cacla and compares the algorithm to other continuous action algorithms.
</p>

<p>
  The output of the actor of Cacla can in principle be any continuous action value or action vector. However, also when these values get rounded to a limited, finite action space, Cacla can outperform discrete algorithms such as <a href="Q.html">Q-learning</a>. See the first publication below for details. 
</p>

<p>
<b>Selected relevant publications:</b>
<ul>
  <li> Hado van Hasselt and Marco Wiering (2009). <a href="../papers/Using_Continuous_Action_Spaces_to_Solve_Discrete_Problems.pdf">"Using Continuous Action Spaces to Solve Discrete Problems"</a>. Proceedings of the International Joint Conference on Neural Networks, IJCNN 2009, Atlanta, GA, USA, 2009.</li>
  <li> Hado van Hasselt and Marco Wiering (2007). "<a href="../papers/Reinforcement_Learning_in_Continuous_Action_Spaces.pdf">Reinforcement Learning in Continuous Action Spaces</a>". Proceedings of IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning (ADPRL07), Honolulu, HI, USA, pp. 272-279, 2007.</li>
</ul>
</p>

<h3>Quick links:</h3>
<a href="Acla.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- Next
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
