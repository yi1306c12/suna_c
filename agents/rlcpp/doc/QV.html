<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>QV-learning</h2>
<a href="alsoV.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="AC.html">Next</a>

<p>
QV-learning is a natural extension of Q-learning and Sarsa to the case where we also use state values. Its equation is:
</p>

<p>
<img src="../img/QV.png" alt="Q_{t+1}(s_t,a_t) \overset{\alpha_t}{\longleftarrow} r_t + \gamma V_t(s_{t+1})" />
</p>

<h4>Neutral characteristics</h4>
<p>
<ul>
  <li>It is on-policy.</li>
  <li>It learns state-action values (Q values).</li>
  <!--<li>It is less optimistic in nature than Q-learning (which can help convergence in some problems).</li>-->
</ul>
</p>

<h4>Advantages</h4>
<p>
<ul>
  <li>Using state values decreases the variance compared to Q-learning and Sarsa.</li>
  <li>Using state values often speeds up learning.</li>
  <li>State values are easily extendable to eligibility traces.</li>
</ul>
</p>

<h4>Disadvantages</h4>
<p>
<ul>
  <li>Cannot handle continuous action spaces.</li>
</ul>
</p>

<a name="alg"></a>
<h4>Algorithm</h4>

<p>
  The QV-learning algorithm in schematic form:
</p>

<p>
  <img src="../img/QValg.png" alt="QV-learning algorithm" />
</p>

<p>
  Comparing this algorithm to <a href="Q.html#alg">Q-learning</a> we see that the state value takes the place of the value of the highest valued action. Amongst other things, this makes the algorithm on-policy and it makes it easier to use eligibility traces.
</p>

<p>
<b>Selected relevant publications:</b>
<li>
  <ul> M.A. Wiering. QV(lambda)-learning: A New On-policy Reinforcement Learning Algorithm. Proceedings of the 7th European Workshop on Reinforcement Learning, D. Leone (editor), pages 17-18, 2005.</ul>
  <ul> M.A. Wiering and H. van Hasselt.  Ensemble Algorithms in Reinforcement Learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B, Volume 38, 4, 930-936, 2008.</ul>
  <ul>M.A. Wiering and H. van Hasselt.  The QV Family Compared to Other Reinforcement Learning Algorithms. Proceedings of IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning (ADPRL), Nashville, USA, 2009.</ul>
</li>
</p>

<h3>Quick links:</h3>
<a href="alsoV.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="AC.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
