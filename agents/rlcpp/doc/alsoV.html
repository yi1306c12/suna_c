<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>State and State-Action Values</h2>
<a href="ESarsa.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="QV.html">Next</a>

<p>
This section discusses algorithms that store the value of a state, in addition to a value for each action that helps determine which action to choose. In that sense, all the algorithms in this section could be called Actor-Critic methods, although we reserve that name for the version that was coined in the book by Sutton and Barto. Note that here a state-action value need not correspond to the expected future rewards, except in that the action with the highest expected reward should normally also receive the highest state-action value. 
</p>

<p>
All algorithms in this section use the following update rule to update the state values:
</p>

<p>
<a name="TDrule">
<img src="../img/V.png" alt="V_{t+1}(s_t)     \overset{\beta_t}{\longleftarrow}  r_t + \gamma V_t(s_{t+1})" /></a>
</p>

<p>
Note that this equation is easily extended to eligibility traces and will in general become more reliable more quickly than Q values, because the state space is smaller than the combined state-action space. Especially in problems with many actions, this can be a big advantage.
</p>

<p>
The algorithms in this section:
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
<a href="Cacla.html">Cacla</a> also uses state values, but can be found in the section containing the continuous action algorithms.
</p>


<h3>Quick links:</h3>
<a href="ESarsa.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="QV.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
