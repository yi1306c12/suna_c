<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>Introduction</h2>
Previous -- <a href="rl_algs.html">Up</a> -- <a href="QuickLinks.html">Next</a>

<p>
This page aims to give a short introduction to some well known and less well known reinforcement learning algorithms. In the near future (i.e. summer of 2009), also implementations (in Python and/or C++) of these algorithms will be included.
</p>

<p>
This page intends to reflect my personal experience with and knowledge about these algorithms. Therefore, it is undoubtably incomplete. If you feel some of the information on this page is incorrect, or something important is missing, please contact me. This page will probably be a continuing work in progress, so the information may be subject change and restructuring.
</p>

<h4>What?</h4>

<p>
This page mainly focuses on the different algorithms and is mainly aimed at researchers and developers that are interested in the different available options. All these algorithms can be used to solve Markov Decision Processes (MDPs) with arbitrary (bounded) reward and transition functions. For an introduction to reinforcement learning and the types of problems that fall into this category, I refer to the excellent book by <a href="http://www.cs.ualberta.ca/~sutton/book/the-book.html">Sutton and Barto</a>.
</p>

<p>
All the algorithms on this page use some type of value functions. This means that algorithms that search directly in policy space are not included at this time. Also, all the algorithms are model free, which implies that for instance dynamic programming as a technique is missing. I may or may not include these later.
</p>

<p>
All the pages in this section have a section with selected relevant publications. Usually these sections only contain two or three papers were more information can be found. These lists are not complete. If you feel important references are missing at some points, please contact me and I'll be happy to comsider adding these. However, I aim to keep the number of references at a minimum to avoid giving pointers in too many directions at once.
</p>

<h4>Why?</h4>

<p>
This page intends to give an overview of some of the available options. It has been shown that in some cases the conventional algorithms do not reach the same performance as some of the newer options. Also, in ensemble methods, one may wish to include different types of algorithms and indeed we have shown combinations of agents that use different algorithms usually perform better than combinations of agents that use the same algorithm. As a final point, we believe it to be useful to investigate the properties of a variety of algorithms in order to facilitate the selection of a good algorithm for a problem, based on an analysis of the problem and the known characteristics of the algorithms instead of empirically trying many options or - even worse - just going with the best known algorithm.
</p>

<p>
<b>Selected relevant publications:</b>
<ul>
  <li> R.S. Sutton and A.G. Barto (1998), Reinforcement Learning: An Introduction, MIT Press, 1998. </li>
  <li> Marco Wiering and Hado van Hasselt (2008). "<a href="../papers/ensemble_RL_final.pdf">Ensemble Algorithms in Reinforcement Learning</a>". In: IEEE Transactions, SMC Part B, special issue on Adaptive Dynamic Programming and Reinforcement Learning in Feedback Control. August 2008. pp. 930-936.</li>
</ul>
</p>

<h3>Quick links:</h3>
Previous -- <a href="rl_algs.html">Up</a> -- <a href="QuickLinks.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>


<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
