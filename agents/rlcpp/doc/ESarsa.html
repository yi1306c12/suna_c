<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<h+tml xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content="text/html;charset=ISO-8859-1" />
  <meta name="keywords" content="hado van hasselt,van hasselt,reinforcement learning,algorithms,q-learning,sarsa,acla,cacla,actor-critic,actor,critic,r-learning,qv-learning,qv,adp,dynamic programming,machine learning,marco wiering,convergence,continuous actions,continuous action spaces"/>
  <title>A Short Introduction To Some RL Algorithms - Hado van Hasselt</title>
  <link href="style.css" rel="stylesheet" media="screen" />
</head>
<body><h1>A Short Introduction To Some Reinforcement Learning Algorithms</h1>
<p>By <a href="../index.html">Hado van Hasselt</a></p>

<h2>Expected-Sarsa</h2>
<a href="Sarsa.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="alsoV.html">Next</a>

<p>
Expected-Sarsa is a variation of Sarsa. Its equation is:
</p>

<p>
<img src="../img/ESarsa.png" alt="Q_{t+1}(s_t,a_t) \overset{\alpha_t}{\longleftarrow} r_t + \gamma \sum_a \pi_t(s_{t+1},a) Q_t(s_{t+1},a)" />
</p>

<p>
The difference is that Expected-Sarsa weighs the action values in the next state according to the current action selection policy. Because this retains the same bias but reduces the variance, the algorithm can be viewed as an improvement over Sarsa. Because it is very similar to Sarsa, its properties are also similar, so we only list the advantages:
</p>

<h4>Advantages over Sarsa</h4>
<p>
<ul>
  <li>Does not have to know the next action to update its value.</li>
  <li>Has lower variance with the same bias.</li>
</ul>
</p>

<a name="alg"></a>
<h4>Algorithm</h4>

<p>
  The Expected-Sarsa algorithm in schematic form:
</p>

<p>
  <img src="../img/ESarsaalg.png" alt="Sarsa algorithm" />
</p>

<p>
  Note that the algorithm is simpler than that of <a href="Sarsa.html#alg">Sarsa</a> and more similar in structure to <a href="Q.html#alg">Q-learning</a>. The algorithm as given does require calculating the policy twice: once before and once after updating the state-action values. Of course, one could also decide to do this only once and reuse the policy that is calculated for the update of the Q-value for the selection of the next action. This results in a slightly different algorithm, which 1) in practice will perform almost exactly the same and 2) also converges theoretically.
</p>

<p>
<b>Selected relevant publications:</b>
<ul>
  <li> H.H. van Seijen, H. van Hasselt, S. Whiteson, and M.A. Wiering (2009). A Theoretical and Empirical Analysis of Expected Sarsa. In Proceedings of the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL'09). </li>
</ul>
</p>

<h3>Quick links:</h3>
<a href="Sarsa.html">Previous</a> -- <a href="rl_algs.html">Up</a> -- <a href="alsoV.html">Next</a>
<p>
  <ul>
    <li><a href="Notation.html">Notation</a></li>
    <li>Using only <a href="onlyQ.html">state-action values</a>:</li>
    <ul>
      <li><a href="Q.html">Q-learning</a></li>
      <li><a href="Sarsa.html">Sarsa</a></li>
      <li><a href="ESarsa.html">Expected-Sarsa</a></li>
    </ul>
    <li>Also using <a href="alsoV.html">state values</a>:</li>
    <ul>
      <li><a href="QV.html">QV-learning</a></li>
      <li><a href="AC.html">Actor-Critic</a></li>
      <li><a href="Acla.html">Acla</a></li>
    </ul>
    <li>Using continuous actions:</li>
    <ul>
      <li><a href="Cacla.html">Cacla</a></li>
    </ul>
  </ul>
</p>

<h3>Contact</h3>

<p>
My contact data can be found on <a href="http://www.cs.uu.nl/staff/hado.html">the staff page</a> of the department.
</p>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-2239771-1";
urchinTracker();
</script>
</body>
</html>
